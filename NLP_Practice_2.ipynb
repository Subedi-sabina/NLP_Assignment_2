{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e20c7e7-4984-4ab5-9c4d-5b5a4fbe62aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b3a9f0-bf43-43a9-a506-38b432e76907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and POS Tags:\n",
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumps: VBZ\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the tokens with their POS tags\n",
    "print(\"Tokens and POS Tags:\")\n",
    "for token, tag in pos_tags:\n",
    "    print(f\"{token}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c4477c-34d7-4136-9844-e09c1ff2248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hello, World! This is a sample sentence with punctuation.\n",
      "Cleaned Sentence: Hello World This is a sample sentence with punctuation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Hello, World! This is a sample sentence with punctuation.\"\n",
    "\n",
    "# Remove special characters and punctuation\n",
    "cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Cleaned Sentence:\", cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df97a321-a21e-4076-8b60-a20f78655d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is a sample sentence demonstrating stop word removal.\n",
      "Filtered Tokens: ['sample', 'sentence', 'demonstrating', 'stop', 'word', 'removal', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is a sample sentence demonstrating stop word removal.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1c09b8-62d0-44a1-920d-1f8e557c8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'runs', 'ran', 'easily', 'fairly']\n",
      "Stemmed Words: ['run', 'run', 'ran', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93359b25-b9f3-4eb2-9161-ac10f61c4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'runs', 'ran', 'easily', 'fairly']\n",
      "Lemmatized Words: ['run', 'run', 'ran', 'easily', 'fairly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Function to get part of speech for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33358aa2-1d9a-4a29-b70e-929312a3b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original HTML: <html><head><title>Title</title></head><body><p>This is a <b>sample</b> paragraph.</p></body></html>\n",
      "Cleaned Text: TitleThis is a sample paragraph.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample HTML text\n",
    "html_text = \"<html><head><title>Title</title></head><body><p>This is a <b>sample</b> paragraph.</p></body></html>\"\n",
    "\n",
    "# Remove HTML tags\n",
    "soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "cleaned_text = soup.get_text()\n",
    "\n",
    "print(\"Original HTML:\", html_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e7708f-c909-49b3-910c-ae8473616226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: There are 123 apples and 456 oranges.\n",
      "Cleaned Sentence: There are  apples and  oranges.\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sentence = \"There are 123 apples and 456 oranges.\"\n",
    "\n",
    "# Remove numbers\n",
    "cleaned_sentence = re.sub(r'\\d+', '', sentence)\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Cleaned Sentence:\", cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "873138f8-e279-4ad0-b10a-e19f8eefbe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is a sample sentence for tokenization.\n",
      "Tokens: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is a sample sentence for tokenization.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e49df70-ca67-4cb0-a3bf-98e56059964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: I love this product! It's amazing.\n",
      "Text After HTML Removal: i love this product its amazing\n",
      "Tokens: ['i', 'love', 'this', 'product', 'its', 'amazing']\n",
      "Filtered Tokens: ['love', 'product', 'amazing']\n",
      "Stemmed Tokens: ['love', 'product', 'amaz']\n",
      "Lemmatized Tokens: ['love', 'product', 'amaze']\n",
      "--------------------------------------------------\n",
      "Original Document: <html><head><title>Title</title></head><body><p>This is a <b>sample</b> paragraph.</p></body></html>\n",
      "Text After HTML Removal: titlethis is a sample paragraph\n",
      "Tokens: ['titlethis', 'is', 'a', 'sample', 'paragraph']\n",
      "Filtered Tokens: ['titlethis', 'sample', 'paragraph']\n",
      "Stemmed Tokens: ['titlethi', 'sampl', 'paragraph']\n",
      "Lemmatized Tokens: ['titlethis', 'sample', 'paragraph']\n",
      "--------------------------------------------------\n",
      "Original Document: He bought 123 apples and 456 oranges.\n",
      "Text After HTML Removal: he bought  apples and  oranges\n",
      "Tokens: ['he', 'bought', 'apples', 'and', 'oranges']\n",
      "Filtered Tokens: ['bought', 'apples', 'oranges']\n",
      "Stemmed Tokens: ['bought', 'appl', 'orang']\n",
      "Lemmatized Tokens: ['bought', 'apple', 'orange']\n",
      "--------------------------------------------------\n",
      "Original Document: Running is a great way to stay fit.\n",
      "Text After HTML Removal: running is a great way to stay fit\n",
      "Tokens: ['running', 'is', 'a', 'great', 'way', 'to', 'stay', 'fit']\n",
      "Filtered Tokens: ['running', 'great', 'way', 'stay', 'fit']\n",
      "Stemmed Tokens: ['run', 'great', 'way', 'stay', 'fit']\n",
      "Lemmatized Tokens: ['run', 'great', 'way', 'stay', 'fit']\n",
      "--------------------------------------------------\n",
      "Original Document: The quick brown fox jumps over the lazy dog.\n",
      "Text After HTML Removal: the quick brown fox jumps over the lazy dog\n",
      "Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Filtered Tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Stemmed Tokens: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "Lemmatized Tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "--------------------------------------------------\n",
      "Original Document: She sells sea shells by the sea shore.\n",
      "Text After HTML Removal: she sells sea shells by the sea shore\n",
      "Tokens: ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']\n",
      "Filtered Tokens: ['sells', 'sea', 'shells', 'sea', 'shore']\n",
      "Stemmed Tokens: ['sell', 'sea', 'shell', 'sea', 'shore']\n",
      "Lemmatized Tokens: ['sell', 'sea', 'shell', 'sea', 'shore']\n",
      "--------------------------------------------------\n",
      "Original Document: COVID-19 has impacted global economies significantly.\n",
      "Text After HTML Removal: covid has impacted global economies significantly\n",
      "Tokens: ['covid', 'has', 'impacted', 'global', 'economies', 'significantly']\n",
      "Filtered Tokens: ['covid', 'impacted', 'global', 'economies', 'significantly']\n",
      "Stemmed Tokens: ['covid', 'impact', 'global', 'economi', 'significantli']\n",
      "Lemmatized Tokens: ['covid', 'impact', 'global', 'economy', 'significantly']\n",
      "--------------------------------------------------\n",
      "Original Document: The new iPhone 13 features a sleek design and powerful performance.\n",
      "Text After HTML Removal: the new iphone  features a sleek design and powerful performance\n",
      "Tokens: ['the', 'new', 'iphone', 'features', 'a', 'sleek', 'design', 'and', 'powerful', 'performance']\n",
      "Filtered Tokens: ['new', 'iphone', 'features', 'sleek', 'design', 'powerful', 'performance']\n",
      "Stemmed Tokens: ['new', 'iphon', 'featur', 'sleek', 'design', 'power', 'perform']\n",
      "Lemmatized Tokens: ['new', 'iphone', 'feature', 'sleek', 'design', 'powerful', 'performance']\n",
      "--------------------------------------------------\n",
      "Original Document: Artificial intelligence and machine learning are transforming industries.\n",
      "Text After HTML Removal: artificial intelligence and machine learning are transforming industries\n",
      "Tokens: ['artificial', 'intelligence', 'and', 'machine', 'learning', 'are', 'transforming', 'industries']\n",
      "Filtered Tokens: ['artificial', 'intelligence', 'machine', 'learning', 'transforming', 'industries']\n",
      "Stemmed Tokens: ['artifici', 'intellig', 'machin', 'learn', 'transform', 'industri']\n",
      "Lemmatized Tokens: ['artificial', 'intelligence', 'machine', 'learn', 'transform', 'industry']\n",
      "--------------------------------------------------\n",
      "Original Document: HTML, CSS, and JavaScript are essential technologies for web development.\n",
      "Text After HTML Removal: html css and javascript are essential technologies for web development\n",
      "Tokens: ['html', 'css', 'and', 'javascript', 'are', 'essential', 'technologies', 'for', 'web', 'development']\n",
      "Filtered Tokens: ['html', 'css', 'javascript', 'essential', 'technologies', 'web', 'development']\n",
      "Stemmed Tokens: ['html', 'css', 'javascript', 'essenti', 'technolog', 'web', 'develop']\n",
      "Lemmatized Tokens: ['html', 'cs', 'javascript', 'essential', 'technology', 'web', 'development']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the Porter Stemmer and WordNet Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample corpus\n",
    "corpus =[     \"I love this product! It's amazing.\",     \"<html><head><title>Title</title></head><body><p>This is a <b>sample</b> paragraph.</p></body></html>\",     \"He bought 123 apples and 456 oranges.\",     \"Running is a great way to stay fit.\",     \"The quick brown fox jumps over the lazy dog.\",     \"She sells sea shells by the sea shore.\",     \"COVID-19 has impacted global economies significantly.\", \"The new iPhone 13 features a sleek design and powerful performance.\", \"Artificial intelligence and machine learning are transforming industries.\", \"HTML, CSS, and JavaScript are essential technologies for web development.\" ]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokens': tokens,\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'stemmed_tokens': stemmed_tokens,\n",
    "        'lemmatized_tokens': lemmatized_tokens\n",
    "    }\n",
    "\n",
    "# Function to get the part of speech tag for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Process each document in the corpus\n",
    "for document in corpus:\n",
    "    result = preprocess_text(document)\n",
    "    print(\"Original Document:\", document)\n",
    "    print(\"Text After HTML Removal:\", result['original'])\n",
    "    print(\"Tokens:\", result['tokens'])\n",
    "    print(\"Filtered Tokens:\", result['filtered_tokens'])\n",
    "    print(\"Stemmed Tokens:\", result['stemmed_tokens'])\n",
    "    print(\"Lemmatized Tokens:\", result['lemmatized_tokens'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b88f90-ee97-482f-91c5-66bd0d503c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
